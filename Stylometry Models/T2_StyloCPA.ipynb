{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDk6_rLx0M-p"
   },
   "source": [
    "## Phraseology Features"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "p7uDJfq9pGpz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0aea8582-313c-4a72-8b1f-cab1a6fbe2f4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Xt3haxT0YnO"
   },
   "source": [
    "### Word Count, Sentence Count, Word count in a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EGDAtWW0LXw",
    "outputId": "029ee9ac-5a41-41c1-817e-40e1e39d3e10"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def word_count(document):\n",
    "\n",
    "  tokens = word_tokenize(document)\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  return len(filtered)\n",
    "\n",
    "def sentence_count(document):\n",
    "\n",
    "  tokens = sent_tokenize(document)\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  return len(filtered)\n",
    "\n",
    "def paragraph_count(document):\n",
    "\n",
    "  tokens = document.splitlines()\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  return len(filtered)\n",
    "\n",
    "def word_count_sent(document):\n",
    "\n",
    "  tokens = sent_tokenize(document)\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  word_counts = [word_count(sent) for sent in filtered]\n",
    "\n",
    "  if len(word_counts) ==0:\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "  mean = sum(word_counts) / len(word_counts)\n",
    "  variance = sum([((x - mean) ** 2) for x in word_counts]) / len(word_counts)\n",
    "  res = variance ** 0.5\n",
    "\n",
    "  return mean, res\n",
    "\n",
    "def word_count_para(document):\n",
    "\n",
    "  tokens = document.splitlines()\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  word_counts = [word_count(para) for para in filtered]\n",
    "\n",
    "  if len(word_counts) ==0:\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "  mean = sum(word_counts) / len(word_counts)\n",
    "  variance = sum([((x - mean) ** 2) for x in word_counts]) / len(word_counts)\n",
    "  res = variance ** 0.5\n",
    "\n",
    "  return mean, res\n",
    "\n",
    "def sent_count_para(document):\n",
    "\n",
    "  tokens = document.splitlines()\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  sent_counts = [sentence_count(para) for para in filtered]\n",
    "\n",
    "  if len(sent_counts) ==0:\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "  mean = sum(sent_counts) / len(sent_counts)\n",
    "  variance = sum([((x - mean) ** 2) for x in sent_counts]) / len(sent_counts)\n",
    "  res = variance ** 0.5\n",
    "\n",
    "  return mean, res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPauOUlIke2c"
   },
   "source": [
    "## Lexical Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjlVgbVOH55c"
   },
   "source": [
    "### Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TNuDGhcH55d",
    "outputId": "8b9dba36-74a8-41fd-c5b4-954edb4ff46c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting py-readability-metrics\n",
      "  Downloading py_readability_metrics-1.4.5-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from py-readability-metrics) (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->py-readability-metrics) (1.15.0)\n",
      "Installing collected packages: py-readability-metrics\n",
      "Successfully installed py-readability-metrics-1.4.5\n",
      "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "!pip install py-readability-metrics\n",
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBfqauKlH55d"
   },
   "outputs": [],
   "source": [
    "from readability import Readability\n",
    "\n",
    "def readability_score(document):\n",
    "\n",
    "  r = Readability(document)\n",
    "\n",
    "  fk = r.flesch_kincaid()\n",
    "  f = r.flesch()\n",
    "  ari = r.ari()\n",
    "\n",
    "  return fk.score, f.score, ari.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlMzMWOUHzWX"
   },
   "source": [
    "### Richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBuBC7COkg-Y",
    "outputId": "505f26cd-bc16-4ac9-bac4-23fb7346af44"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting lexicalrichness\n",
      "  Downloading lexicalrichness-0.1.4.tar.gz (18 kB)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from lexicalrichness) (1.4.1)\n",
      "Requirement already satisfied: textblob>=0.15.3 in /usr/local/lib/python3.7/dist-packages (from lexicalrichness) (0.15.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.0.0->lexicalrichness) (1.21.6)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob>=0.15.3->lexicalrichness) (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob>=0.15.3->lexicalrichness) (1.15.0)\n",
      "Building wheels for collected packages: lexicalrichness\n",
      "  Building wheel for lexicalrichness (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for lexicalrichness: filename=lexicalrichness-0.1.4-py2.py3-none-any.whl size=10109 sha256=8a1bfe22246c55a407601faf494b0304ce1ad2332456a2abff4246bdc6f54f96\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/53/09/ce0a119b59493ae5be4e9773457df832bbce66d926fce1d043\n",
      "Successfully built lexicalrichness\n",
      "Installing collected packages: lexicalrichness\n",
      "Successfully installed lexicalrichness-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install lexicalrichness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbZJZxCjknVn"
   },
   "outputs": [],
   "source": [
    "from lexicalrichness import LexicalRichness\n",
    "sample_size = 10\n",
    "iterations = 50 \n",
    "\n",
    "def lexical_richness(document):\n",
    "  \n",
    "  lex = LexicalRichness(document)\n",
    "  ret_list = []\n",
    "  words = document.split()\n",
    "  if len(words)>45:\n",
    "    ret_list.append(lex.mattr(window_size=25))\n",
    "  else:\n",
    "    ret_list.append(lex.mattr(window_size=len(words)//3))\n",
    "  ret_list.append(lex.mtld(threshold=0.72))\n",
    "  return ret_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rh8VKbwrQza"
   },
   "source": [
    "## Punctuation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcMPvG-vrVuA"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def total_punc_count(document):\n",
    "  \n",
    "  punct_count = 0\n",
    "\n",
    "  for char in document:\n",
    "    \n",
    "    if char in string.punctuation:\n",
    "\n",
    "      punct_count +=1\n",
    "  \n",
    "  return punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVsRrdwozUPC"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def special_punc_count(document, special_puncts):\n",
    "  \n",
    "  punct_count = []\n",
    "\n",
    "  for punct in special_puncts:\n",
    "    \n",
    "    punct_count.append(document.count(punct))\n",
    "  \n",
    "  total_puncts = total_punc_count(document)\n",
    "  if total_puncts==0:\n",
    "    return [0 for count in punct_count]\n",
    "  else:\n",
    "    return [float(count)/ total_puncts for count in punct_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgQPaoUGCihb",
    "outputId": "ef1f18af-cabd-47f6-ef1a-e7fb146d3a58"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "def special_punc_count_sent(document, special_puncts):\n",
    "\n",
    "  tokens = sent_tokenize(document)\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  punct_count = [0 for i in special_puncts] # Init as 0 \n",
    "\n",
    "  if not filtered:\n",
    "    return punct_count\n",
    "\n",
    "  for sent in filtered:\n",
    "\n",
    "    for punct in special_puncts:\n",
    "      \n",
    "      punct_count[special_puncts.index(punct)] += sent.count(punct)\n",
    "    \n",
    "  return [float(count)/ len(filtered) for count in punct_count]\n",
    "\n",
    "\n",
    "def special_punc_count_para(document, special_puncts):\n",
    "\n",
    "  tokens = document.splitlines()\n",
    "\n",
    "  nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "  \n",
    "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "  punct_count = [0 for i in special_puncts] # Init as 0 \n",
    "\n",
    "  if not filtered:\n",
    "    return punct_count\n",
    "\n",
    "  for para in filtered:\n",
    "\n",
    "    for punct in special_puncts:\n",
    "      \n",
    "      punct_count[special_puncts.index(punct)] += para.count(punct)\n",
    "    \n",
    "  return [float(count)/ len(filtered) for count in punct_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imO_l7G4Gbm8"
   },
   "source": [
    "## Feature Exatrction"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined function to get phraseology, lexical and punctuation features"
   ],
   "metadata": {
    "id": "hGgMcA-cpYSk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features(data):\n",
    "\n",
    "  data_features = []\n",
    "\n",
    "  phraseology_features = [\"word_count\", \"sent_count\", \"para_count\", \"mean_word_count_sent\", \"std_word_count_sent\", \"mean_word_count_para\", \"std_word_count_para\", \"mean_sent_count_para\", \"std_sent_count_para\"]\n",
    "  # diversity_features = [\"fk_score\", \"f_score\", \"ari_score\", \"mattr\", \"mtld\"]\n",
    "  diversity_features = [\"mattr\", \"mtld\"]\n",
    "  special_puncts = [\"!\",\"'\", \",\", \"-\", \":\", \";\", \"?\", \"@\", \"\\\"\", \"=\", \"#\"]\n",
    "\n",
    "  special_punct_names = [\"excla\",\"apos\", \"comma\", \"hypn\", \"col\", \"semicol\", \"ques\", \"at\", \"qot\", \"dhypn\", \"hash\"]\n",
    "\n",
    "  punct_analysis_features = [\"total_punct_count\"]\n",
    "\n",
    "  for punct in special_punct_names:\n",
    "\n",
    "    punct_analysis_features.append(punct + \"_mean_count\")\n",
    "\n",
    "  for punct in special_punct_names:\n",
    "\n",
    "    punct_analysis_features.append(punct + \"_mean_count_sent\")\n",
    "\n",
    "  for punct in special_punct_names:\n",
    "\n",
    "    punct_analysis_features.append(punct + \"_mean_count_para\")\n",
    "\n",
    "\n",
    "  for value in (data.itertuples()):\n",
    "\n",
    "    document = str(value.text)\n",
    "\n",
    "    if not document:\n",
    "\n",
    "      document = \"empty\"\n",
    "\n",
    "\n",
    "    feature_row = []\n",
    "    ## phraseology features\n",
    "    # print(document)\n",
    "    feature_row.append(word_count(document))\n",
    "    feature_row.append(sentence_count(document))\n",
    "    feature_row.append(paragraph_count(document))\n",
    "\n",
    "    # word count per sentence\n",
    "\n",
    "    word_count_vals = word_count_sent(document)\n",
    "    feature_row.append(word_count_vals[0])\n",
    "    feature_row.append(word_count_vals[1])\n",
    "\n",
    "    # word count per paragraph\n",
    "    word_count_vals = word_count_para(document)\n",
    "    feature_row.append(word_count_vals[0])\n",
    "    feature_row.append(word_count_vals[1])\n",
    "\n",
    "    # sentence count per paragraph\n",
    "    sent_count_vals = sent_count_para(document)\n",
    "    feature_row.append(sent_count_vals[0])\n",
    "    feature_row.append(sent_count_vals[1])\n",
    "\n",
    "    ## diversity features\n",
    "\n",
    "    reareadability = readability_score(document)\n",
    "    feature_row.append(reareadability[0])\n",
    "    feature_row.append(reareadability[1])\n",
    "    feature_row.append(reareadability[2])\n",
    "\n",
    "    word count per sentence\n",
    "    richness = lexical_richness(document)\n",
    "    feature_row.append(richness[0])\n",
    "    feature_row.append(richness[1])\n",
    "\n",
    "\n",
    "    ## punctuation features\n",
    "\n",
    "    feature_row.append(total_punc_count(document))\n",
    "    feature_row.extend(special_punc_count(document, special_puncts))\n",
    "    feature_row.extend(special_punc_count_sent(document, special_puncts))\n",
    "    feature_row.extend(special_punc_count_para(document, special_puncts))\n",
    "\n",
    "\n",
    "    # append label\n",
    "    #feature_row.append(value.label)\n",
    "    data_features.append(feature_row)\n",
    "\n",
    "  frame_cols = phraseology_features\n",
    "  frame_cols.extend(diversity_features)\n",
    "  frame_cols.extend(punct_analysis_features)\n",
    "  # frame_cols.append('label')\n",
    "\n",
    "  # print(\"length of feature vector (column names) \")\n",
    "  # print(len(frame_cols))\n",
    "\n",
    "  data_features = pd.DataFrame(data_features, columns=frame_cols)\n",
    "  return data_features\n"
   ],
   "metadata": {
    "id": "2dnchUVepuvQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "id": "937D7qZgrH2S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ruptures\n",
    "import ruptures as rpt"
   ],
   "metadata": {
    "id": "GPd-Hz0EBpTU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_timeline_mix(data, agreement_threshold=0.2, count_threshold=1):\n",
    "\n",
    "  index = 0 \n",
    "\n",
    "  tp = 0\n",
    "  tn = 0\n",
    "  fn = 0\n",
    "  fp = 0 \n",
    "  \n",
    "  results_csv = []\n",
    "  results_cols = [\"index\", \"llr\"]\n",
    "\n",
    "  for value in tqdm(data.itertuples()):\n",
    "\n",
    "    text = value.text\n",
    "    \n",
    "    output = ast.literal_eval(text)\n",
    "\n",
    "    frame = pd.DataFrame(output, columns = [\"text\"])\n",
    "\n",
    "    timeline_features = get_features(frame)\n",
    "\n",
    "    if index == 0:\n",
    "\n",
    "      feature_cols = timeline_features.columns\n",
    "\n",
    "    change_point_feature_count = 0 \n",
    "\n",
    "    for col in feature_cols:\n",
    "\n",
    "      ts1 = timeline_features[[col]].values\n",
    "\n",
    "      cpd_algo = rpt.Pelt(model=\"rbf\").fit(ts1)\n",
    "      change_locations = cpd_algo.predict(pen=1)\n",
    "\n",
    "      if len(change_locations) > count_threshold:\n",
    "        change_point_feature_count +=1\n",
    "    \n",
    "    if float(change_point_feature_count)/len(feature_cols) > agreement_threshold:\n",
    "\n",
    "      llr = -1\n",
    "      label = 0\n",
    "    \n",
    "    else:\n",
    "\n",
    "      llr = 1\n",
    "      label = 1\n",
    "    \n",
    "    results_csv.append([index, llr])\n",
    "\n",
    "    tp += ((label == value.label) & (value.label == 1))\n",
    "    tn += ((label == value.label) & (value.label == 0))\n",
    "    fn += ((label != value.label) & (value.label == 1))\n",
    "    fp += ((label != value.label) & (value.label == 0))\n",
    "\n",
    "    index +=1\n",
    "\n",
    "    recall = float(tp) / (tp+fn)\n",
    "    precision = float(tp) / (tp+fp)\n",
    "    f1_score = 2 * float(precision) * recall / (precision + recall)\n",
    "\n",
    "\n",
    "  print('TP: %d' % (\n",
    "      tp))\n",
    "  print('TN: %d' % (\n",
    "      tn))\n",
    "  print('FP: %d' % (\n",
    "      fp))\n",
    "  print('FN: %d' % (\n",
    "      fn))\n",
    "\n",
    "  accuracy = 100 * (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "  print(accuracy)\n",
    "\n",
    "  results_frame = pd.DataFrame(results_csv, columns=results_cols)\n",
    "\n",
    "  return results_frame, accuracy"
   ],
   "metadata": {
    "id": "xB5KLydjeQ0_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "def predict_changepoint(data, w=0):\n",
    "\n",
    "  index = 0 \n",
    "\n",
    "  tp = 0\n",
    "  \n",
    "  results_csv = []\n",
    "  results_cols = [\"index\", \"llr\"]\n",
    "\n",
    "  for value in tqdm(data.itertuples()):\n",
    "\n",
    "    text = value.text\n",
    "    \n",
    "    output = ast.literal_eval(text)\n",
    "\n",
    "    frame = pd.DataFrame(output, columns = [\"text\"])\n",
    "\n",
    "    timeline_features = get_features(frame)\n",
    "\n",
    "    if index == 0:\n",
    "\n",
    "      feature_cols = timeline_features.columns\n",
    "\n",
    "    change_point_feature_count = 0 \n",
    "    change_point_index = []\n",
    "\n",
    "    for col in feature_cols:\n",
    "\n",
    "      ts1 = timeline_features[[col]].values\n",
    "\n",
    "      cpd_algo = rpt.Pelt(model=\"rbf\").fit(ts1)\n",
    "      change_locations = cpd_algo.predict(pen=1)\n",
    "\n",
    "      if len(change_locations) > count_threshold:\n",
    "        change_point_feature_count +=1\n",
    "        change_point_index.extend(change_locations)\n",
    "    \n",
    "    if float(change_point_feature_count)/len(feature_cols) > agreement_threshold:  # There exisits a change point \n",
    "\n",
    "      pred_idx = most_frequent(change_point_index)\n",
    "    \n",
    "    else:\n",
    "\n",
    "      pred_idx = -1\n",
    "  \n",
    "    tp += ((pred_idx <= value.index + w) || (pred_idx >= value.index + w))\n",
    "  \n",
    "    index +=1\n",
    "\n",
    "  accuracy = 100 * (tp) / (index)\n",
    "\n",
    "  print(accuracy)\n",
    "\n",
    "  return accuracy"
   ],
   "metadata": {
    "id": "M0JpjDnDCCOg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "predict_timeline_mix(test_data, agreement_threshold=0.15, count_threshold=2)\n",
    "predict_changepoint(test_data, agreement_threshold=0.15, count_threshold=2)"
   ],
   "metadata": {
    "id": "ufFZLfDnfYE8"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "T2: StyloCPA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}