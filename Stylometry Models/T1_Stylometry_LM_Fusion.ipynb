{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQ9aqjHjt9a7",
    "outputId": "4f374938-2a35-4443-93f3-daba11bf6732"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stylometry + LM"
   ],
   "metadata": {
    "id": "tY2z9FLtU8rU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Requirements"
   ],
   "metadata": {
    "id": "GHlVDq4EdrMO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install numpy==1.19.4\n",
    "!pip install PyYAML>=5.4\n",
    "!pip install spacy==2.2.4\n",
    "!pip install torch==1.7.0\n",
    "!pip install torchtext==0.3.1\n",
    "!pip install tqdm==4.53.0\n",
    "!pip install pandas==1.1.5\n",
    "!pip install transformers==4.3.2\n",
    "!pip install fire==0.4.0\n",
    "!pip install requests==2.23.0\n",
    "!pip install tensorboard==2.4.1\n",
    "!pip install download==0.3.5\n",
    "!pip install nltk>=3.6.6\n",
    "\n",
    "!pip install py-readability-metrics\n",
    "!python -m nltk.downloader punkt\n",
    "!pip install lexicalrichness"
   ],
   "metadata": {
    "id": "VnUbCQLHdv8D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "id": "Km_X3ft1e4n2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from functools import reduce\n",
    "\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "def summary(model: nn.Module, file=sys.stdout):\n",
    "    def repr(model):\n",
    "        # We treat the extra repr like the sub-module, one item per line\n",
    "        extra_lines = []\n",
    "        extra_repr = model.extra_repr()\n",
    "        # empty string will be split into list ['']\n",
    "        if extra_repr:\n",
    "            extra_lines = extra_repr.split('\\n')\n",
    "        child_lines = []\n",
    "        total_params = 0\n",
    "        for key, module in model._modules.items():\n",
    "            mod_str, num_params = repr(module)\n",
    "            mod_str = nn.modules.module._addindent(mod_str, 2)\n",
    "            child_lines.append('(' + key + '): ' + mod_str)\n",
    "            total_params += num_params\n",
    "        lines = extra_lines + child_lines\n",
    "\n",
    "        for name, p in model._parameters.items():\n",
    "            if hasattr(p, 'shape'):\n",
    "                total_params += reduce(lambda x, y: x * y, p.shape)\n",
    "\n",
    "        main_str = model._get_name() + '('\n",
    "        if lines:\n",
    "            # simple one-liner info, which most builtin Modules will use\n",
    "            if len(extra_lines) == 1 and not child_lines:\n",
    "                main_str += extra_lines[0]\n",
    "            else:\n",
    "                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "\n",
    "        main_str += ')'\n",
    "        if file is sys.stdout:\n",
    "            main_str += ', \\033[92m{:,}\\033[0m params'.format(total_params)\n",
    "        else:\n",
    "            main_str += ', {:,} params'.format(total_params)\n",
    "        return main_str, total_params\n",
    "\n",
    "    string, count = repr(model)\n",
    "    if file is not None:\n",
    "        if isinstance(file, str):\n",
    "            file = open(file, 'w')\n",
    "        print(string, file=file)\n",
    "        file.flush()\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def grad_norm(model: nn.Module):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "\n",
    "def distributed():\n",
    "    return dist.is_available() and dist.is_initialized()"
   ],
   "metadata": {
    "id": "dX42ci5Le7jn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extractor"
   ],
   "metadata": {
    "id": "kUuhX_s2qI0_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "import string \n",
    "\n",
    "from lexicalrichness import LexicalRichness\n",
    "from readability import Readability\n",
    "\n",
    "class Stylometry():\n",
    "\n",
    "  def __init__(self, phraseology_features= True, diversity_features = True, punct_analysis_features = True):\n",
    "\n",
    "    self.phraseology_features = phraseology_features\n",
    "    self.diversity_features = diversity_features\n",
    "    self.punct_analysis_features = punct_analysis_features\n",
    "\n",
    "  def word_count(self, document):\n",
    "\n",
    "    tokens = word_tokenize(document)\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    return len(filtered)\n",
    "\n",
    "  def sentence_count(self, document):\n",
    "\n",
    "    tokens = sent_tokenize(document)\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    return len(filtered)\n",
    "\n",
    "  def paragraph_count(self, document):\n",
    "\n",
    "    tokens = document.splitlines()\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    return len(filtered)\n",
    "\n",
    "  def word_count_sent(self, document):\n",
    "\n",
    "    tokens = sent_tokenize(document)\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    word_counts = [self.word_count(sent) for sent in filtered]\n",
    "\n",
    "    if len(word_counts) ==0:\n",
    "\n",
    "      return 0, 0\n",
    "\n",
    "    mean = sum(word_counts) / len(word_counts)\n",
    "    variance = sum([((x - mean) ** 2) for x in word_counts]) / len(word_counts)\n",
    "    res = variance ** 0.5\n",
    "\n",
    "    return mean, res\n",
    "\n",
    "  def word_count_para(self, document):\n",
    "\n",
    "    tokens = document.splitlines()\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    word_counts = [self.word_count(para) for para in filtered]\n",
    "\n",
    "    if len(word_counts) ==0:\n",
    "\n",
    "      return 0, 0\n",
    "\n",
    "    mean = sum(word_counts) / len(word_counts)\n",
    "    variance = sum([((x - mean) ** 2) for x in word_counts]) / len(word_counts)\n",
    "    res = variance ** 0.5\n",
    "\n",
    "    return mean, res\n",
    "\n",
    "  def sent_count_para(self, document):\n",
    "\n",
    "    tokens = document.splitlines()\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    sent_counts = [self.sentence_count(para) for para in filtered]\n",
    "\n",
    "    if len(sent_counts) ==0:\n",
    "\n",
    "      return 0, 0\n",
    "\n",
    "    mean = sum(sent_counts) / len(sent_counts)\n",
    "    variance = sum([((x - mean) ** 2) for x in sent_counts]) / len(sent_counts)\n",
    "    res = variance ** 0.5\n",
    "\n",
    "    return mean, res\n",
    "\n",
    "\n",
    "  def total_punc_count(self, document):\n",
    "    \n",
    "    punct_count = 0\n",
    "\n",
    "    for char in document:\n",
    "      \n",
    "      if char in string.punctuation:\n",
    "\n",
    "        punct_count +=1\n",
    "    \n",
    "    return punct_count\n",
    "\n",
    "\n",
    "  def special_punc_count(self, document, special_puncts):\n",
    "    \n",
    "    punct_count = []\n",
    "\n",
    "    for punct in special_puncts:\n",
    "      \n",
    "      punct_count.append(document.count(punct))\n",
    "    \n",
    "    total_puncts = self.total_punc_count(document)\n",
    "    if total_puncts==0:\n",
    "      return [0 for count in punct_count]\n",
    "    else:\n",
    "      return [float(count)/ total_puncts for count in punct_count]\n",
    "\n",
    "  def special_punc_count_sent(self, document, special_puncts):\n",
    "\n",
    "    tokens = sent_tokenize(document)\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    punct_count = [0 for i in special_puncts] # Init as 0 \n",
    "\n",
    "    if not filtered:\n",
    "      return punct_count\n",
    "\n",
    "    for sent in filtered:\n",
    "\n",
    "      for punct in special_puncts:\n",
    "        \n",
    "        punct_count[special_puncts.index(punct)] += sent.count(punct)\n",
    "      \n",
    "    return [float(count)/ len(filtered) for count in punct_count]\n",
    "\n",
    "\n",
    "  def special_punc_count_para(self, document, special_puncts):\n",
    "\n",
    "    tokens = document.splitlines()\n",
    "\n",
    "    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "    \n",
    "    filtered = [w for w in tokens if nonPunct.match(w)]\n",
    "\n",
    "    punct_count = [0 for i in special_puncts] # Init as 0 \n",
    "\n",
    "    if not filtered:\n",
    "      return punct_count\n",
    "\n",
    "    for para in filtered:\n",
    "\n",
    "      for punct in special_puncts:\n",
    "        \n",
    "        punct_count[special_puncts.index(punct)] += para.count(punct)\n",
    "      \n",
    "    return [float(count)/ len(filtered) for count in punct_count]\n",
    "\n",
    "  \n",
    "  def readability_score(self, document):\n",
    "\n",
    "    try: \n",
    "\n",
    "      r = Readability(document)\n",
    "\n",
    "      fk = r.flesch_kincaid()\n",
    "      f = r.flesch()\n",
    "      ari = r.ari()\n",
    "\n",
    "    except:\n",
    "\n",
    "      return 0, 0, 0\n",
    "    \n",
    "    else:\n",
    "      \n",
    "      return fk.score, f.score, ari.score\n",
    "\n",
    "\n",
    "  def lexical_richness(self, document):\n",
    "\n",
    "    sample_size = 10\n",
    "    iterations = 50 \n",
    "    \n",
    "    lex = LexicalRichness(document)\n",
    "    ret_list = []\n",
    "    words = document.split()\n",
    "    if len(words)>45:\n",
    "      ret_list.append(lex.mattr(window_size=25))\n",
    "    else:\n",
    "      ret_list.append(lex.mattr(window_size=len(words)//3))\n",
    "    ret_list.append(lex.mtld(threshold=0.72))\n",
    "    return ret_list\n",
    "\n",
    "  \n",
    "  def get_features(self, document, special_puncts):\n",
    "\n",
    "    feature_row = [] \n",
    "\n",
    "    if self.phraseology_features:\n",
    "      ## phraseology features\n",
    "      # print(document)\n",
    "      feature_row.append(self.word_count(document))\n",
    "      feature_row.append(self.sentence_count(document))\n",
    "      feature_row.append(self.paragraph_count(document))\n",
    "\n",
    "      # word count per sentence\n",
    "\n",
    "      word_count_vals = self.word_count_sent(document)\n",
    "      feature_row.append(word_count_vals[0])\n",
    "      feature_row.append(word_count_vals[1])\n",
    "\n",
    "      # word count per paragraph\n",
    "      word_count_vals = self.word_count_para(document)\n",
    "      feature_row.append(word_count_vals[0])\n",
    "      feature_row.append(word_count_vals[1])\n",
    "\n",
    "      # sentence count per paragraph\n",
    "      sent_count_vals = self.sent_count_para(document)\n",
    "      feature_row.append(sent_count_vals[0])\n",
    "      feature_row.append(sent_count_vals[1])\n",
    "\n",
    "    if self.diversity_features:\n",
    "      # diversity features\n",
    "\n",
    "      reareadability = self.readability_score(document)\n",
    "      feature_row.append(reareadability[0])\n",
    "      feature_row.append(reareadability[1])\n",
    "      feature_row.append(reareadability[2])\n",
    "\n",
    "      # word count per sentence\n",
    "      richness = self.lexical_richness(document)\n",
    "      feature_row.append(richness[0])\n",
    "      feature_row.append(richness[1])\n",
    "\n",
    "    if self.punct_analysis_features:\n",
    "      ## punctuation features\n",
    "\n",
    "      feature_row.append(self.total_punc_count(document))\n",
    "      feature_row.extend(self.special_punc_count(document, special_puncts))\n",
    "      feature_row.extend(self.special_punc_count_sent(document, special_puncts))\n",
    "      feature_row.extend(self.special_punc_count_para(document, special_puncts))\n",
    "\n",
    "    return feature_row"
   ],
   "metadata": {
    "id": "gRIJX_HiqNC_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6c4e69ed-b647-4670-df64-3dbe3826c77e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loader"
   ],
   "metadata": {
    "id": "tMMwHRMpdc_K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "# from pycontractions import Contractions\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "CONTRACTION_MAP = { \"ain't\": \"is not\",\n",
    "                    \"aren't\": \"are not\",\n",
    "                    \"can't\": \"cannot\",\n",
    "                    \"can't've\": \"cannot have\",\n",
    "                    \"'cause\": \"because\",\n",
    "                    \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\",\n",
    "                    \"couldn't've\": \"could not have\",\n",
    "                    \"didn't\": \"did not\",\n",
    "                    \"doesn't\": \"does not\",\n",
    "                    \"don't\": \"do not\",\n",
    "                    \"hadn't\": \"had not\",\n",
    "                    \"hadn't've\": \"had not have\",\n",
    "                    \"hasn't\": \"has not\",\n",
    "                    \"haven't\": \"have not\",\n",
    "                    \"he'd\": \"he would\",\n",
    "                    \"he'd've\": \"he would have\",\n",
    "                    \"he'll\": \"he will\",\n",
    "                    \"he'll've\": \"he he will have\",\n",
    "                    \"he's\": \"he is\",\n",
    "                    \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\",\n",
    "                    \"how'll\": \"how will\",\n",
    "                    \"how's\": \"how is\",\n",
    "                    \"I'd\": \"I would\",\n",
    "                    \"I ain't\": \"I am not\",\n",
    "                    \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\",\n",
    "                    \"I'll've\": \"I will have\",\n",
    "                    \"I'm\": \"I am\",\n",
    "                    \"I've\": \"I have\",\n",
    "                    \"i'd\": \"i would\",\n",
    "                    \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",\n",
    "                    \"i'll've\": \"i will have\",\n",
    "                    \"i'm\": \"i am\",\n",
    "                    \"i've\": \"i have\",\n",
    "                    \"isn't\": \"is not\",\n",
    "                    \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\",\n",
    "                    \"it'll\": \"it will\",\n",
    "                    \"it'll've\": \"it will have\",\n",
    "                    \"it's\": \"it is\",\n",
    "                    \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\",\n",
    "                    \"mayn't\": \"may not\",\n",
    "                    \"might've\": \"might have\",\n",
    "                    \"mightn't\": \"might not\",\n",
    "                    \"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\",\n",
    "                    \"mustn't\": \"must not\",\n",
    "                    \"mustn't've\": \"must not have\",\n",
    "                    \"needn't\": \"need not\",\n",
    "                    \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\",\n",
    "                    \"oughtn't\": \"ought not\",\n",
    "                    \"oughtn't've\": \"ought not have\",\n",
    "                    \"shan't\": \"shall not\",\n",
    "                    \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\",\n",
    "                    \"she'd\": \"she would\",\n",
    "                    \"she'd've\": \"she would have\",\n",
    "                    \"she'll\": \"she will\",\n",
    "                    \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\",\n",
    "                    \"should've\": \"should have\",\n",
    "                    \"shouldn't\": \"should not\",\n",
    "                    \"shouldn't've\": \"should not have\",\n",
    "                    \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\",\n",
    "                    \"that'd\": \"that would\",\n",
    "                    \"that'd've\": \"that would have\",\n",
    "                    \"that's\": \"that is\",\n",
    "                    \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\",\n",
    "                    \"there's\": \"there is\",\n",
    "                    \"they'd\": \"they would\",\n",
    "                    \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\",\n",
    "                    \"they'll've\": \"they will have\",\n",
    "                    \"they're\": \"they are\",\n",
    "                    \"they've\": \"they have\",\n",
    "                    \"to've\": \"to have\",\n",
    "                    \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\",\n",
    "                    \"we'd've\": \"we would have\",\n",
    "                    \"we'll\": \"we will\",\n",
    "                    \"we'll've\": \"we will have\",\n",
    "                    \"we're\": \"we are\",\n",
    "                    \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\",\n",
    "                    \"what'll\": \"what will\",\n",
    "                    \"what'll've\": \"what will have\",\n",
    "                    \"what're\": \"what are\",\n",
    "                    \"what's\": \"what is\",\n",
    "                    \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\",\n",
    "                    \"when've\": \"when have\",\n",
    "                    \"where'd\": \"where did\",\n",
    "                    \"where's\": \"where is\",\n",
    "                    \"where've\": \"where have\",\n",
    "                    \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\",\n",
    "                    \"who's\": \"who is\",\n",
    "                    \"who've\": \"who have\",\n",
    "                    \"why's\": \"why is\",\n",
    "                    \"why've\": \"why have\",\n",
    "                    \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\",\n",
    "                    \"won't've\": \"will not have\",\n",
    "                    \"would've\": \"would have\",\n",
    "                    \"wouldn't\": \"would not\",\n",
    "                    \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\",\n",
    "                    \"y'all'd\": \"you all would\",\n",
    "                    \"y'all'd've\": \"you all would have\",\n",
    "                    \"y'all're\": \"you all are\",\n",
    "                    \"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\",\n",
    "                    \"you'd've\": \"you would have\",\n",
    "                    \"you'll\": \"you will\",\n",
    "                    \"you'll've\": \"you will have\",\n",
    "                    \"you're\": \"you are\",\n",
    "                    \"you've\": \"you have\"\n",
    "                    }\n",
    "\n",
    "\n",
    "class PreProcess:\n",
    "    def __init__(self, lowercase_norm=False, period_norm=False, special_chars_norm=False, accented_norm=False, contractions_norm=False,\n",
    "                 stemming_norm=False, lemma_norm=False, stopword_norm=False, proper_norm=False):\n",
    "\n",
    "        self.lowercase_norm = lowercase_norm\n",
    "        self.period_norm = period_norm\n",
    "        self.special_chars_norm = special_chars_norm\n",
    "        self.accented_norm = accented_norm\n",
    "        self.contractions_norm = contractions_norm\n",
    "        self.stemming_norm = stemming_norm\n",
    "        self.lemma_norm = lemma_norm\n",
    "        self.stopword_norm = stopword_norm\n",
    "        self.proper_norm = proper_norm\n",
    "\n",
    "    def lowercase_normalization(self, data):\n",
    "\n",
    "        return data.lower()\n",
    "\n",
    "    def period_remove(self, data):\n",
    "\n",
    "        return data.replace(\".\", \" \")\n",
    "\n",
    "    def special_char_remove(self, data, remove_digits=False):  # Remove special characters\n",
    "        tokens = self.tokenization(data)\n",
    "        special_char_norm_data = []\n",
    "\n",
    "        for token in tokens:\n",
    "            sentence = \"\"\n",
    "            for word in token:\n",
    "                sentence += word + \" \"\n",
    "            sentence.rstrip()\n",
    "\n",
    "            clean_remove = re.compile('<.*?>')\n",
    "            norm_sentence = re.sub(clean_remove, '', sentence)\n",
    "\n",
    "            norm_sentence = re.sub(r'[^\\x00-\\x7F]+','', norm_sentence)\n",
    "            norm_sentence = norm_sentence.replace(\"\\\\\", \"\")\n",
    "            norm_sentence = norm_sentence.replace(\"-\", \" \")\n",
    "            norm_sentence = norm_sentence.replace(\",\", \"\")\n",
    "            special_char_norm_data.append(norm_sentence)\n",
    "\n",
    "        return special_char_norm_data\n",
    "\n",
    "    def accented_word_normalization(self, data):  # Normalize accented chars/words\n",
    "        tokens = self.tokenization(data)\n",
    "        accented_norm_data = []\n",
    "\n",
    "        for token in tokens:\n",
    "            sentence = \"\"\n",
    "            for word in token:\n",
    "                sentence += word + \" \"\n",
    "            sentence.rstrip()\n",
    "            norm_sentence = unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "            accented_norm_data.append(norm_sentence)\n",
    "\n",
    "        return accented_norm_data\n",
    "\n",
    "    def expand_contractions(self, data, pycontrct=False):  # Expand contractions\n",
    "\n",
    "        # Simple contraction removal based on pre-defined set of contractions\n",
    "        contraction_mapping = CONTRACTION_MAP\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                          flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match) \\\n",
    "                if contraction_mapping.get(match) \\\n",
    "                else contraction_mapping.get(match.lower())\n",
    "            expanded_contraction = first_char + expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        tokens = self.tokenization(data)\n",
    "        contraction_norm_data = []\n",
    "\n",
    "        for token in tokens:\n",
    "            sentence = \"\"\n",
    "            for word in token:\n",
    "                sentence += word + \" \"\n",
    "            sentence.rstrip()\n",
    "\n",
    "            expanded_text = contractions_pattern.sub(expand_match, sentence)\n",
    "            expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "\n",
    "            contraction_norm_data.append(expanded_text)\n",
    "\n",
    "        return contraction_norm_data\n",
    "\n",
    "    def stemming(self, data):\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens = self.tokenization(data)\n",
    "        stemmed_data = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            s1 = \" \".join(stemmer.stem(tokens[i][j]) for j in range(len(tokens[i])))\n",
    "            stemmed_data.append(s1)\n",
    "\n",
    "        return stemmed_data\n",
    "\n",
    "    def lemmatization(self, data):\n",
    "        lemma = nltk.stem.WordNetLemmatizer()\n",
    "        tokens = self.tokenization(data)\n",
    "        lemmatized_data = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            s1 = \" \".join(lemma.lemmatize(tokens[i][j]) for j in range(len(tokens[i])))\n",
    "            lemmatized_data.append(s1)\n",
    "\n",
    "        return lemmatized_data\n",
    "\n",
    "    def stopword_remove(self, data):  # Remove special characters\n",
    "        filtered_sentence = []\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        data = self.tokenization(data)\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            res = \"\"\n",
    "            for j in range(len(data[i])):\n",
    "                if data[i][j].lower() not in stop_words:\n",
    "                    res = res + \" \" + data[i][j]\n",
    "            filtered_sentence.append(res)\n",
    "\n",
    "        return filtered_sentence\n",
    "\n",
    "    def remove_proper_nouns(self, data):\n",
    "        common_words = []\n",
    "        data = self.tokenization(data)\n",
    "        for i in range(len(data)):\n",
    "            tagged_sent = pos_tag(data[i])\n",
    "            proper_nouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
    "            res = \"\"\n",
    "            for j in range(len(data[i])):\n",
    "                if data[i][j] not in proper_nouns:\n",
    "                    res = res + \" \" + data[i][j]\n",
    "            common_words.append(res)\n",
    "\n",
    "        return common_words\n",
    "\n",
    "    def tokenization(self, data):\n",
    "        tokens = []\n",
    "        for i in range(len(data)):\n",
    "            tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "            tokens.append(tokenizer.tokenize(data[i]))\n",
    "        return tokens\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        data = [str(data)]\n",
    "\n",
    "        if self.special_chars_norm:\n",
    "            data = self.special_char_remove(data, remove_digits=False)\n",
    "\n",
    "        # if self.contractions_norm:\n",
    "        #     data = self.expand_contractions(data)\n",
    "\n",
    "        if self.accented_norm:\n",
    "            data = self.accented_word_normalization(data)\n",
    "\n",
    "        if self.stemming_norm:\n",
    "            data = self.stemming(data)\n",
    "\n",
    "        if self.proper_norm:\n",
    "            data = self.remove_proper_nouns(data)\n",
    "\n",
    "        if self.stopword_norm:\n",
    "            data = self.stopword_remove(data)\n",
    "\n",
    "        if self.lemma_norm:\n",
    "            data = self.lemmatization(data)\n",
    "\n",
    "        data = data[0]\n",
    "\n",
    "        if self.lowercase_norm:\n",
    "            data = self.lowercase_normalization(str(data))\n",
    "\n",
    "        if self.period_norm:\n",
    "            data = self.period_remove(str(data))\n",
    "\n",
    "        return data\n",
    "\n",
    "def load_texts(data_file, label=False, expected_size=None):\n",
    "    texts = []\n",
    "\n",
    "    for line in tqdm(open(data_file), desc=f'Loading {data_file}'):\n",
    "        texts.append(json.loads(line)['text'])\n",
    "\n",
    "    if label:\n",
    "        label = []\n",
    "        for line in tqdm(open(data_file), desc=f'Loading {data_file}'):\n",
    "            label.append(json.loads(line)['label'])\n",
    "\n",
    "        return texts, label\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, name, data_dir='data', label=False, skip_train=False, single_file=False):\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        if single_file:\n",
    "\n",
    "            if label:\n",
    "                self.data, self.label = load_texts(f'{data_dir}/{name}.jsonl', label=True)\n",
    "            else:\n",
    "                self.data = load_texts(f'{data_dir}/{name}.jsonl')\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.train = load_texts(f'{data_dir}/{name}.train.jsonl') if not skip_train else None\n",
    "            self.test = load_texts(f'{data_dir}/{name}.test.jsonl')\n",
    "            self.valid = load_texts(f'{data_dir}/{name}.valid.jsonl')\n",
    "\n",
    "\n",
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, real_texts: List[str], fake_texts: List[str], tokenizer: PreTrainedTokenizer, special_puncts: List[str],\n",
    "                 max_sequence_length: int = None, min_sequence_length: int = None):\n",
    "        self.real_texts = real_texts\n",
    "        self.fake_texts = fake_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.min_sequence_length = min_sequence_length\n",
    "        self.special_puncts = special_puncts\n",
    "        self.style_extractor= Stylometry(phraseology_features= True, diversity_features = False, punct_analysis_features = True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.real_texts) + len(self.fake_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if index < len(self.real_texts):\n",
    "            text = self.real_texts[index]\n",
    "            label = 0\n",
    "        else:\n",
    "            text = self.fake_texts[index - len(self.real_texts)]\n",
    "            label = 1\n",
    "\n",
    "        stylo_features = self.style_extractor.get_features(text, self.special_puncts)\n",
    "        # Preprocessing\n",
    "        preprocessor = PreProcess(special_chars_norm=True, lowercase_norm=True, period_norm=True, proper_norm=True, accented_norm=True)\n",
    "\n",
    "        text = preprocessor.fit(text)\n",
    "\n",
    "        padded_sequences = self.tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "        \n",
    "        return torch.tensor(padded_sequences['input_ids']), torch.tensor(padded_sequences['attention_mask']), torch.tensor(stylo_features), label\n",
    "\n",
    "\n",
    "\n",
    "class EncodedSingleDataset(Dataset):\n",
    "    def __init__(self, input_texts: List[str], input_labels: List[int], tokenizer: PreTrainedTokenizer, special_puncts: List[str],\n",
    "                 max_sequence_length: int = None, min_sequence_length: int = None):\n",
    "        self.input_texts = input_texts\n",
    "        self.input_labels = input_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.min_sequence_length = min_sequence_length\n",
    "        self.special_puncts = special_puncts\n",
    "        self.style_extractor= Stylometry(phraseology_features= True, diversity_features = False, punct_analysis_features = True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = self.input_texts[index]\n",
    "        label = self.input_labels[index]\n",
    "\n",
    "        stylo_features = self.style_extractor.get_features(text, self.special_puncts)\n",
    "\n",
    "        # Preprocessing\n",
    "        preprocessor = PreProcess(special_chars_norm=True, lowercase_norm=True, period_norm=True, proper_norm=True, accented_norm=True)\n",
    "\n",
    "        text = preprocessor.fit(text)\n",
    "\n",
    "        padded_sequences = self.tokenizer(text, padding='max_length', max_length=self.max_sequence_length, truncation=True)\n",
    "\n",
    "\n",
    "        return torch.tensor(padded_sequences['input_ids']), torch.tensor(padded_sequences['attention_mask']), torch.tensor(stylo_features), label\n",
    "\n",
    "\n",
    "class EncodeEvalData(Dataset):\n",
    "    def __init__(self, input_texts: List[str], tokenizer: PreTrainedTokenizer, special_puncts: List[str],\n",
    "                 max_sequence_length: int = None, min_sequence_length: int = None):\n",
    "\n",
    "        self.input_texts = input_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.min_sequence_length = min_sequence_length\n",
    "        self.special_puncts = special_puncts\n",
    "        self.style_extractor= Stylometry(phraseology_features= True, diversity_features = False, punct_analysis_features = True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.input_texts[index]\n",
    "\n",
    "        stylo_features = self.style_extractor.get_features(text, self.special_puncts)\n",
    "        # Preprocessing\n",
    "        preprocessor = PreProcess(special_chars_norm=True, lowercase_norm=True, period_norm=True, proper_norm=True, accented_norm=True)\n",
    "\n",
    "        text = preprocessor.fit(text)\n",
    "\n",
    "        padded_sequences = self.tokenizer(text, padding='max_length', max_length=self.max_sequence_length, truncation=True)\n",
    "\n",
    "\n",
    "        return torch.tensor(padded_sequences['input_ids']), torch.tensor(padded_sequences['attention_mask']), torch.tensor(stylo_features)\n"
   ],
   "metadata": {
    "id": "1htLTvXkdhpJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ebc0b288-daec-4f3f-ef2c-76229b5fada2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Code"
   ],
   "metadata": {
    "id": "GoPm9R_FeV5j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Roberta Model"
   ],
   "metadata": {
    "id": "go5H4TfpeZ1u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SequenceClassifierOutputWithLastLayer(SequenceClassifierOutput):\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "class RobertaForFusion(RobertaForSequenceClassification):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.soft_max = Softmax(dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                else:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        softmax_logits = self.soft_max(logits)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (softmax_logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithLastLayer(\n",
    "            loss=loss,\n",
    "            logits=softmax_logits,\n",
    "            last_hidden_state=sequence_output,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n"
   ],
   "metadata": {
    "id": "7bClPgQKeYO-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joined Ensemble (Stylo + LM)"
   ],
   "metadata": {
    "id": "u2CzReQteeb-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FusedClassifier(torch.nn.Module):\n",
    "    def __init__(self, lm, device, FUSED_INPUT_SIZE):\n",
    "        super(FusedClassifier, self).__init__()\n",
    "\n",
    "        self.lm = lm\n",
    "\n",
    "        # move to device\n",
    "        self.lm.to(device)\n",
    "\n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Linear(FUSED_INPUT_SIZE, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64)\n",
    "        ).to(device)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device)\n",
    "\n",
    "        # the LM is already pre-trained, no need to calc grads anymore\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, data, custom_features):\n",
    "        # output = self.BERT(data[0].to(device), attention_mask=data[2].to(device))\n",
    "        # output = output[-1][0][:, -1, :].detach()\n",
    "\n",
    "        if len(data) < 3:\n",
    "          output_dic = self.lm(data[0], attention_mask=data[1])\n",
    "        \n",
    "        else:\n",
    "          output_dic = self.lm(data[0], attention_mask=data[1], labels=data[2])\n",
    "\n",
    "        lm_emb_output = output_dic[\"last_hidden_state\"][:, -1, :].detach()\n",
    "\n",
    "        # append manuall features to Roberta features\n",
    "        x = torch.cat((lm_emb_output, custom_features), axis=-1)\n",
    "        c = self.reducer(x)\n",
    "\n",
    "        return self.classifier(c)"
   ],
   "metadata": {
    "id": "NtRD1A9f55Xw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Code"
   ],
   "metadata": {
    "id": "oo22No0JfGE-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LM Attribution Fine-Tuning"
   ],
   "metadata": {
    "id": "Nd2k_imVfLuD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"Training code for the detector model\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from itertools import count\n",
    "from multiprocessing import Process\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, DistributedSampler, RandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import *\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(int(1000))\n",
    "\n",
    "def setup_distributed(port=29500):\n",
    "    if not dist.is_available() or not torch.cuda.is_available() or torch.cuda.device_count() <= 1:\n",
    "        return 0, 1\n",
    "\n",
    "    if 'MPIR_CVAR_CH3_INTERFACE_HOSTNAME' in os.environ:\n",
    "        from mpi4py import MPI\n",
    "        mpi_rank = MPI.COMM_WORLD.Get_rank()\n",
    "        mpi_size = MPI.COMM_WORLD.Get_size()\n",
    "\n",
    "        os.environ[\"MASTER_ADDR\"] = '127.0.0.1'\n",
    "        os.environ[\"MASTER_PORT\"] = str(port)\n",
    "\n",
    "        dist.init_process_group(backend=\"nccl\", world_size=mpi_size, rank=mpi_rank)\n",
    "        return mpi_rank, mpi_size\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "    return dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "\n",
    "def load_datasets(data_dir, real_dataset, fake_dataset, tokenizer, special_puncts, batch_size,\n",
    "                  max_sequence_length, random_sequence_length):\n",
    "\n",
    "    real_corpus = Corpus(real_dataset, data_dir=data_dir)\n",
    "\n",
    "    if fake_dataset == \"TWO\":\n",
    "        real_train, real_valid = real_corpus.train * 2, real_corpus.valid * 2\n",
    "        fake_corpora = [Corpus(name, data_dir=data_dir) for name in ['grover_fake', 'gpt2_fake']]\n",
    "        fake_train = sum([corpus.train for corpus in fake_corpora], [])\n",
    "        fake_valid = sum([corpus.valid for corpus in fake_corpora], [])\n",
    "\n",
    "    else:\n",
    "        fake_corpus = Corpus(fake_dataset, data_dir=data_dir)\n",
    "\n",
    "        real_train, real_valid = real_corpus.train, real_corpus.valid\n",
    "        fake_train, fake_valid = fake_corpus.train, fake_corpus.valid\n",
    "\n",
    "    Sampler = DistributedSampler if distributed() and dist.get_world_size() > 1 else RandomSampler\n",
    "\n",
    "    min_sequence_length = 10 if random_sequence_length else None\n",
    "    train_dataset = EncodedDataset(real_train, fake_train, tokenizer, special_puncts, max_sequence_length, min_sequence_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, sampler=Sampler(train_dataset), num_workers=0)\n",
    "\n",
    "    validation_dataset = EncodedDataset(real_valid, fake_valid, tokenizer, special_puncts, max_sequence_length, min_sequence_length)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=1, sampler=Sampler(validation_dataset))\n",
    "\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "\n",
    "def accuracy_sum(logits, labels):\n",
    "    if list(logits.shape) == list(labels.shape) + [2]:\n",
    "        # 2-d outputs\n",
    "        classification = (logits[..., 0] < logits[..., 1]).long().flatten()\n",
    "    else:\n",
    "        classification = (logits > 0).long().flatten()\n",
    "    assert classification.shape == labels.shape\n",
    "    return (classification == labels).float().sum().item()\n",
    "\n",
    "\n",
    "def train(model: nn.Module, optimizer, device: str, loader: DataLoader, desc='Train'):\n",
    "    model.train()\n",
    "\n",
    "    train_accuracy = 0\n",
    "    train_epoch_size = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    with tqdm(loader, desc=desc, disable=distributed() and dist.get_rank() > 0) as loop:\n",
    "        for texts, masks, _, labels in loop:\n",
    "\n",
    "            texts, masks, labels = texts.to(device), masks.to(device), labels.to(device)\n",
    "            batch_size = texts.shape[0]\n",
    "            # print(texts)\n",
    "            # print(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output_dic = model(texts, attention_mask=masks, labels=labels)\n",
    "            loss, logits = output_dic[\"loss\"], output_dic[\"logits\"]\n",
    "            # print(\"Loss is:\" , model(texts, attention_mask=masks, labels=labels))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_accuracy = accuracy_sum(logits, labels)\n",
    "            train_accuracy += batch_accuracy\n",
    "            train_epoch_size += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=train_accuracy / train_epoch_size)\n",
    "\n",
    "    return {\n",
    "        \"train/accuracy\": train_accuracy,\n",
    "        \"train/epoch_size\": train_epoch_size,\n",
    "        \"train/loss\": train_loss\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(model: nn.Module, device: str, loader: DataLoader, votes=1, desc='Validation'):\n",
    "    model.eval()\n",
    "\n",
    "    validation_accuracy = 0\n",
    "    validation_epoch_size = 0\n",
    "    validation_loss = 0\n",
    "\n",
    "    records = [record for v in range(votes) for record in tqdm(loader, desc=f'Preloading data ... {v}',\n",
    "                                                               disable=distributed() and dist.get_rank() > 0)]\n",
    "    records = [[records[v * len(loader) + i] for v in range(votes)] for i in range(len(loader))]\n",
    "\n",
    "    with tqdm(records, desc=desc, disable=distributed() and dist.get_rank() > 0) as loop, torch.no_grad():\n",
    "        for example in loop:\n",
    "            losses = []\n",
    "            logit_votes = []\n",
    "\n",
    "            for texts, masks, _, labels in example:\n",
    "                texts, masks, labels = texts.to(device), masks.to(device), labels.to(device)\n",
    "                batch_size = texts.shape[0]\n",
    "\n",
    "                output_dic = model(texts, attention_mask=masks, labels=labels)\n",
    "                loss, logits = output_dic[\"loss\"], output_dic[\"logits\"]\n",
    "                losses.append(loss)\n",
    "                logit_votes.append(logits)\n",
    "\n",
    "            loss = torch.stack(losses).mean(dim=0)\n",
    "            logits = torch.stack(logit_votes).mean(dim=0)\n",
    "\n",
    "            batch_accuracy = accuracy_sum(logits, labels)\n",
    "            validation_accuracy += batch_accuracy\n",
    "            validation_epoch_size += batch_size\n",
    "            validation_loss += loss.item() * batch_size\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=validation_accuracy / validation_epoch_size)\n",
    "\n",
    "    return {\n",
    "        \"validation/accuracy\": validation_accuracy,\n",
    "        \"validation/epoch_size\": validation_epoch_size,\n",
    "        \"validation/loss\": validation_loss\n",
    "    }\n",
    "\n",
    "\n",
    "def _all_reduce_dict(d, device):\n",
    "    # wrap in tensor and use reduce to gpu0 tensor\n",
    "    output_d = {}\n",
    "    for (key, value) in sorted(d.items()):\n",
    "        tensor_input = torch.tensor([[value]]).to(device)\n",
    "        # torch.distributed.all_reduce(tensor_input)\n",
    "        output_d[key] = tensor_input.item()\n",
    "    return output_d\n",
    "\n",
    "\n",
    "def run(max_epochs=None,\n",
    "        device=None,\n",
    "        batch_size=8,\n",
    "        max_sequence_length=256,\n",
    "        random_sequence_length=False,\n",
    "        epoch_size=None,\n",
    "        seed=None,\n",
    "        data_dir='data',\n",
    "        real_dataset='real',\n",
    "        fake_dataset='grover_fake',\n",
    "        token_dropout=None,\n",
    "        large=True,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0,\n",
    "        load_from_checkpoint=False,\n",
    "        checkpoint_name='neuralnews',\n",
    "        special_puncts= [],\n",
    "        **kwargs):\n",
    "    args = locals()\n",
    "    rank, world_size = setup_distributed()\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    if device is None:\n",
    "        device = f'cuda:{rank}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print('rank:', rank, 'world_size:', world_size, 'device:', device)\n",
    "\n",
    "    import torch.distributed as dist\n",
    "    if distributed() and rank > 0:\n",
    "        dist.barrier()\n",
    "\n",
    "    model_name = 'roberta-large' if large else 'roberta-base'\n",
    "    tokenization_utils.logger.setLevel('ERROR')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForFusion.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Load the model from checkpoints\n",
    "    if load_from_checkpoint:\n",
    "        if device == \"cpu\":\n",
    "            model.load_state_dict(torch.load((data_dir + '{}.pt').format(checkpoint_name),\n",
    "                                             map_location='cpu')['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(\n",
    "                torch.load((data_dir + '{}.pt').format(checkpoint_name))['model_state_dict'])\n",
    "\n",
    "    if rank == 0:\n",
    "        summary(model)\n",
    "        if distributed():\n",
    "            dist.barrier()\n",
    "\n",
    "    if world_size > 1:\n",
    "        model = DistributedDataParallel(model, [rank], output_device=rank, find_unused_parameters=True)\n",
    "\n",
    "    train_loader, validation_loader = load_datasets(data_dir, real_dataset, fake_dataset, tokenizer, special_puncts, batch_size,\n",
    "                                                    max_sequence_length, random_sequence_length)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    epoch_loop = count(1) if max_epochs is None else range(1, max_epochs + 1)\n",
    "\n",
    "    logdir = os.environ.get(\"OPENAI_LOGDIR\", \"logs\")\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(logdir) if rank == 0 else None\n",
    "    best_validation_accuracy = 0\n",
    "    without_progress = 0\n",
    "    earlystop_epochs = 3\n",
    "\n",
    "    for epoch in epoch_loop:\n",
    "        if world_size > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "            validation_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_metrics = train(model, optimizer, device, train_loader, f'Epoch {epoch}')\n",
    "        validation_metrics = validate(model, device, validation_loader)\n",
    "\n",
    "        combined_metrics = _all_reduce_dict({**validation_metrics, **train_metrics}, device)\n",
    "\n",
    "        combined_metrics[\"train/accuracy\"] /= combined_metrics[\"train/epoch_size\"]\n",
    "        combined_metrics[\"train/loss\"] /= combined_metrics[\"train/epoch_size\"]\n",
    "        combined_metrics[\"validation/accuracy\"] /= combined_metrics[\"validation/epoch_size\"]\n",
    "        combined_metrics[\"validation/loss\"] /= combined_metrics[\"validation/epoch_size\"]\n",
    "\n",
    "        if rank == 0:\n",
    "            for key, value in combined_metrics.items():\n",
    "                writer.add_scalar(key, value, global_step=epoch)\n",
    "\n",
    "            if combined_metrics[\"validation/accuracy\"] > best_validation_accuracy:\n",
    "                without_progress = 0\n",
    "                best_validation_accuracy = combined_metrics[\"validation/accuracy\"]\n",
    "\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                torch.save(dict(\n",
    "                        epoch=epoch,\n",
    "                        model_state_dict=model_to_save.state_dict(),\n",
    "                        optimizer_state_dict=optimizer.state_dict(),\n",
    "                        args=args\n",
    "                    ),\n",
    "                    os.path.join(\"\", \"roberta_ft.pt\")\n",
    "                )\n",
    "\n",
    "        without_progress += 1\n",
    "\n",
    "        if without_progress >= earlystop_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--max-epochs', type=int, default=None)\n",
    "    parser.add_argument('--device', type=str, default=None)\n",
    "    parser.add_argument('--batch-size', type=int, default=16)\n",
    "    parser.add_argument('--max-sequence-length', type=int, default=256)\n",
    "    parser.add_argument('--random-sequence-length', action='store_true')\n",
    "    parser.add_argument('--epoch-size', type=int, default=None)\n",
    "    parser.add_argument('--seed', type=int, default=None)\n",
    "    parser.add_argument('--data-dir', type=str, default=\"\")\n",
    "    parser.add_argument('--real-dataset', type=str, default='')\n",
    "    parser.add_argument('--fake-dataset', type=str, default='')\n",
    "    parser.add_argument('--token-dropout', type=float, default=None)\n",
    "\n",
    "    parser.add_argument('--large', action='store_true', help='use the roberta-large model instead of roberta-base')\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-5)\n",
    "    parser.add_argument('--weight-decay', type=float, default=0)\n",
    "    parser.add_argument('--load-decay', type=float, default=0)\n",
    "    parser.add_argument('--special_puncts', type=list, default=[\"!\",\"'\", \",\", \"-\", \":\", \";\", \"?\", \"@\", \"\\\"\", \"=\", \"#\"])\n",
    "\n",
    "    args = parser.parse_args(args=['--max-epochs=20'])\n",
    "\n",
    "    nproc = int(subprocess.check_output([sys.executable, '-c', \"import torch;\"\n",
    "                                         \"print(torch.cuda.device_count() if torch.cuda.is_available() else 1)\"]))\n",
    "    if nproc > 1:\n",
    "        print(f'Launching {nproc} processes ...', file=sys.stderr)\n",
    "\n",
    "        os.environ[\"MASTER_ADDR\"] = '127.0.0.1'\n",
    "        os.environ[\"MASTER_PORT\"] = str(29500)\n",
    "        os.environ['WORLD_SIZE'] = str(nproc)\n",
    "        os.environ['OMP_NUM_THREAD'] = str(1)\n",
    "        subprocesses = []\n",
    "\n",
    "        for i in range(nproc):\n",
    "            os.environ['RANK'] = str(i)\n",
    "            os.environ['LOCAL_RANK'] = str(i)\n",
    "            process = Process(target=run, kwargs=vars(args))\n",
    "            process.start()\n",
    "            subprocesses.append(process)\n",
    "\n",
    "        for process in subprocesses:\n",
    "            process.join()\n",
    "    else:\n",
    "        run(**vars(args))"
   ],
   "metadata": {
    "id": "zhX0XkNCfJAt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LM + Sylo Fusion Training"
   ],
   "metadata": {
    "id": "7z0ryOiCfT0A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"Training code for the detector model\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from itertools import count\n",
    "from multiprocessing import Process\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, DistributedSampler, RandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import *\n",
    "\n",
    "\n",
    "torch.manual_seed(int(1000))\n",
    "\n",
    "def setup_distributed(port=29500):\n",
    "    if not dist.is_available() or not torch.cuda.is_available() or torch.cuda.device_count() <= 1:\n",
    "        return 0, 1\n",
    "\n",
    "    if 'MPIR_CVAR_CH3_INTERFACE_HOSTNAME' in os.environ:\n",
    "        from mpi4py import MPI\n",
    "        mpi_rank = MPI.COMM_WORLD.Get_rank()\n",
    "        mpi_size = MPI.COMM_WORLD.Get_size()\n",
    "\n",
    "        os.environ[\"MASTER_ADDR\"] = '127.0.0.1'\n",
    "        os.environ[\"MASTER_PORT\"] = str(port)\n",
    "\n",
    "        dist.init_process_group(backend=\"nccl\", world_size=mpi_size, rank=mpi_rank)\n",
    "        return mpi_rank, mpi_size\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "    return dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "\n",
    "def load_datasets(data_dir, real_dataset, fake_dataset, tokenizer, special_puncts, batch_size,\n",
    "                  max_sequence_length, random_sequence_length):\n",
    "\n",
    "    real_corpus = Corpus(real_dataset, data_dir=data_dir)\n",
    "\n",
    "    if fake_dataset == \"TWO\":\n",
    "        real_train, real_valid = real_corpus.train * 2, real_corpus.valid * 2\n",
    "        fake_corpora = [Corpus(name, data_dir=data_dir) for name in ['grover_fake', 'gpt2_fake']]\n",
    "        fake_train = sum([corpus.train for corpus in fake_corpora], [])\n",
    "        fake_valid = sum([corpus.valid for corpus in fake_corpora], [])\n",
    "\n",
    "    else:\n",
    "        fake_corpus = Corpus(fake_dataset, data_dir=data_dir)\n",
    "\n",
    "        real_train, real_valid = real_corpus.train, real_corpus.valid\n",
    "        fake_train, fake_valid = fake_corpus.train, fake_corpus.valid\n",
    "\n",
    "    Sampler = DistributedSampler if distributed() and dist.get_world_size() > 1 else RandomSampler\n",
    "\n",
    "    min_sequence_length = 10 if random_sequence_length else None\n",
    "    train_dataset = EncodedDataset(real_train, fake_train, tokenizer, special_puncts, max_sequence_length, min_sequence_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, sampler=Sampler(train_dataset), num_workers=0)\n",
    "\n",
    "    validation_dataset = EncodedDataset(real_valid, fake_valid, tokenizer, special_puncts, max_sequence_length, min_sequence_length)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=1, sampler=Sampler(validation_dataset))\n",
    "\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_sum(logits, labels):\n",
    "    if list(logits.shape) == list(labels.shape) + [2]:\n",
    "        # 2-d outputs\n",
    "        classification = (logits[..., 0] < logits[..., 1]).long().flatten()\n",
    "    else:\n",
    "        classification = (logits > 0).long().flatten()\n",
    "    assert classification.shape == labels.shape\n",
    "    return (classification == labels).float().sum().item()\n",
    "\n",
    "\n",
    "def train(model: nn.Module, optimizer, device: str, loader: DataLoader, desc='Train'):\n",
    "    model.train()\n",
    "\n",
    "    train_accuracy = 0\n",
    "    train_epoch_size = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    with tqdm(loader, desc=desc, disable=distributed() and dist.get_rank() > 0) as loop:\n",
    "        for texts, masks, custom_features, labels in loop:\n",
    "\n",
    "            texts, masks, custom_features, labels = texts.to(device), masks.to(device), custom_features.to(device), labels.to(device)\n",
    "            batch_size = texts.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predict_label = model(data=[texts, masks, labels], custom_features = custom_features)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(predict_label, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_accuracy = accuracy_sum(predict_label, labels)\n",
    "            train_accuracy += batch_accuracy\n",
    "            train_epoch_size += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=train_accuracy / train_epoch_size)\n",
    "\n",
    "    return {\n",
    "        \"train/accuracy\": train_accuracy,\n",
    "        \"train/epoch_size\": train_epoch_size,\n",
    "        \"train/loss\": train_loss\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(model: nn.Module, device: str, loader: DataLoader, votes=1, desc='Validation'):\n",
    "    model.eval()\n",
    "\n",
    "    validation_accuracy = 0\n",
    "    validation_epoch_size = 0\n",
    "    validation_loss = 0\n",
    "\n",
    "    records = [record for v in range(votes) for record in tqdm(loader, desc=f'Preloading data ... {v}',\n",
    "                                                               disable=distributed() and dist.get_rank() > 0)]\n",
    "    records = [[records[v * len(loader) + i] for v in range(votes)] for i in range(len(loader))]\n",
    "\n",
    "    with tqdm(records, desc=desc, disable=distributed() and dist.get_rank() > 0) as loop, torch.no_grad():\n",
    "        for example in loop:\n",
    "            losses = []\n",
    "            logit_votes = []\n",
    "\n",
    "            for texts, masks, custom_features, labels in example:\n",
    "\n",
    "              texts, masks, custom_features, labels = texts.to(device), masks.to(device), custom_features.to(device), labels.to(device)\n",
    "              batch_size = texts.shape[0]\n",
    "\n",
    "              predict_label = model(data=[texts, masks, labels], custom_features = custom_features)\n",
    "\n",
    "              loss_fct = CrossEntropyLoss()\n",
    "              loss = loss_fct(predict_label, labels)\n",
    "              losses.append(loss)\n",
    "              logit_votes.append(predict_label)\n",
    "\n",
    "            loss = torch.stack(losses).mean(dim=0)\n",
    "            logits = torch.stack(logit_votes).mean(dim=0)\n",
    "\n",
    "            batch_accuracy = accuracy_sum(logits, labels)\n",
    "            validation_accuracy += batch_accuracy\n",
    "            validation_epoch_size += batch_size\n",
    "            validation_loss += loss.item() * batch_size\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=validation_accuracy / validation_epoch_size)\n",
    "\n",
    "    return {\n",
    "        \"validation/accuracy\": validation_accuracy,\n",
    "        \"validation/epoch_size\": validation_epoch_size,\n",
    "        \"validation/loss\": validation_loss\n",
    "    }\n",
    "\n",
    "\n",
    "def _all_reduce_dict(d, device):\n",
    "    # wrap in tensor and use reduce to gpu0 tensor\n",
    "    output_d = {}\n",
    "    for (key, value) in sorted(d.items()):\n",
    "        tensor_input = torch.tensor([[value]]).to(device)\n",
    "        # torch.distributed.all_reduce(tensor_input)\n",
    "        output_d[key] = tensor_input.item()\n",
    "    return output_d\n",
    "\n",
    "\n",
    "def run(max_epochs=None,\n",
    "        device=None,\n",
    "        batch_size=16,\n",
    "        max_sequence_length=256,\n",
    "        random_sequence_length=False,\n",
    "        epoch_size=None,\n",
    "        seed=None,\n",
    "        data_dir='data',\n",
    "        real_dataset='real',\n",
    "        fake_dataset='grover_fake',\n",
    "        token_dropout=None,\n",
    "        large=True,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0,\n",
    "        load_from_checkpoint=False,\n",
    "        checkpoint_name='neuralnews',\n",
    "        special_puncts= [],\n",
    "        FUSED_INPUT_SIZE = 811,\n",
    "        **kwargs):\n",
    "    args = locals()\n",
    "    rank, world_size = setup_distributed()\n",
    "\n",
    "    if device is None:\n",
    "        device = f'cuda:{rank}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print('rank:', rank, 'world_size:', world_size, 'device:', device)\n",
    "\n",
    "    import torch.distributed as dist\n",
    "    if distributed() and rank > 0:\n",
    "        dist.barrier()\n",
    "\n",
    "    model_name = 'roberta-large' if large else 'roberta-base'\n",
    "    tokenization_utils.logger.setLevel('ERROR')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    lm = RobertaForFusion.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Load the model from checkpoints\n",
    "    if load_from_checkpoint:\n",
    "        if device == \"cpu\":\n",
    "            lm.load_state_dict(torch.load((data_dir + '{}.pt').format(checkpoint_name),\n",
    "                                             map_location='cpu')['model_state_dict'])\n",
    "        else:\n",
    "            lm.load_state_dict(\n",
    "                torch.load((data_dir + '{}.pt').format(checkpoint_name))['model_state_dict'])\n",
    "\n",
    "\n",
    "    model = FusedClassifier(lm=lm, device=device, FUSED_INPUT_SIZE=FUSED_INPUT_SIZE)\n",
    "    \n",
    "    if rank == 0:\n",
    "        summary(model)\n",
    "        if distributed():\n",
    "            dist.barrier()\n",
    "\n",
    "    if world_size > 1:\n",
    "        model = DistributedDataParallel(model, [rank], output_device=rank, find_unused_parameters=True)\n",
    "\n",
    "    train_loader, validation_loader = load_datasets(data_dir, real_dataset, fake_dataset, tokenizer, special_puncts, batch_size,\n",
    "                                                    max_sequence_length, random_sequence_length)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    epoch_loop = count(1) if max_epochs is None else range(1, max_epochs + 1)\n",
    "\n",
    "    logdir = os.environ.get(\"OPENAI_LOGDIR\", \"logs\")\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(logdir) if rank == 0 else None\n",
    "    best_validation_accuracy = 0\n",
    "    without_progress = 0\n",
    "    earlystop_epochs = 3\n",
    "\n",
    "    for epoch in epoch_loop:\n",
    "        if world_size > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "            validation_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_metrics = train(model, optimizer, device, train_loader, f'Epoch {epoch}')\n",
    "        validation_metrics = validate(model, device, validation_loader)\n",
    "\n",
    "        combined_metrics = _all_reduce_dict({**validation_metrics, **train_metrics}, device)\n",
    "\n",
    "        combined_metrics[\"train/accuracy\"] /= combined_metrics[\"train/epoch_size\"]\n",
    "        combined_metrics[\"train/loss\"] /= combined_metrics[\"train/epoch_size\"]\n",
    "        combined_metrics[\"validation/accuracy\"] /= combined_metrics[\"validation/epoch_size\"]\n",
    "        combined_metrics[\"validation/loss\"] /= combined_metrics[\"validation/epoch_size\"]\n",
    "\n",
    "        if rank == 0:\n",
    "            for key, value in combined_metrics.items():\n",
    "                writer.add_scalar(key, value, global_step=epoch)\n",
    "\n",
    "            if combined_metrics[\"validation/accuracy\"] > best_validation_accuracy:\n",
    "                without_progress = 0\n",
    "                best_validation_accuracy = combined_metrics[\"validation/accuracy\"]\n",
    "\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                torch.save(dict(\n",
    "                        epoch=epoch,\n",
    "                        model_state_dict=model_to_save.state_dict(),\n",
    "                        optimizer_state_dict=optimizer.state_dict(),\n",
    "                        args=args\n",
    "                    ),\n",
    "                    os.path.join(\"\", \"robertagenattr_fusion_grover.pt\")\n",
    "                )\n",
    "\n",
    "        without_progress += 1\n",
    "\n",
    "        if without_progress >= earlystop_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--max-epochs', type=int, default=None)\n",
    "    parser.add_argument('--device', type=str, default=None)\n",
    "    parser.add_argument('--batch-size', type=int, default=16)\n",
    "    parser.add_argument('--max-sequence-length', type=int, default=256)\n",
    "    parser.add_argument('--random-sequence-length', action='store_true')\n",
    "    parser.add_argument('--epoch-size', type=int, default=None)\n",
    "    parser.add_argument('--seed', type=int, default=None)\n",
    "    parser.add_argument('--data-dir', type=str, default=\"\")\n",
    "    parser.add_argument('--real-dataset', type=str, default='')\n",
    "    parser.add_argument('--fake-dataset', type=str, default='')\n",
    "    parser.add_argument('--token-dropout', type=float, default=None)\n",
    "\n",
    "    parser.add_argument('--large', action='store_true', help='use the roberta-large model instead of roberta-base')\n",
    "    parser.add_argument('--learning-rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--weight-decay', type=float, default=0)\n",
    "    parser.add_argument('--load-decay', type=float, default=0)\n",
    "\n",
    "    parser.add_argument('--special_puncts', type=list, default=[\"!\",\"'\", \",\", \"-\", \":\", \";\", \"?\", \"@\", \"\\\"\", \"=\", \"#\"])\n",
    "    parser.add_argument('--FUSED_INPUT_SIZE', type=int, default=811)\n",
    "\n",
    "    args = parser.parse_args(args=['--max-epochs=20'])\n",
    "\n",
    "    nproc = int(subprocess.check_output([sys.executable, '-c', \"import torch;\"\n",
    "                                         \"print(torch.cuda.device_count() if torch.cuda.is_available() else 1)\"]))\n",
    "    if nproc > 1:\n",
    "        print(f'Launching {nproc} processes ...', file=sys.stderr)\n",
    "\n",
    "        os.environ[\"MASTER_ADDR\"] = '127.0.0.1'\n",
    "        os.environ[\"MASTER_PORT\"] = str(29500)\n",
    "        os.environ['WORLD_SIZE'] = str(nproc)\n",
    "        os.environ['OMP_NUM_THREAD'] = str(1)\n",
    "        subprocesses = []\n",
    "\n",
    "        for i in range(nproc):\n",
    "            os.environ['RANK'] = str(i)\n",
    "            os.environ['LOCAL_RANK'] = str(i)\n",
    "            process = Process(target=run, kwargs=vars(args))\n",
    "            process.start()\n",
    "            subprocesses.append(process)\n",
    "\n",
    "        for process in subprocesses:\n",
    "            process.join()\n",
    "    else:\n",
    "        run(**vars(args))"
   ],
   "metadata": {
    "id": "zWdnBc_Olnlr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "id": "PE77SnV-bK_j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import decimal\n",
    "\n",
    "\n",
    "from transformers import *\n",
    "\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "    while start < stop:\n",
    "        yield float(start)\n",
    "        start += decimal.Decimal(step)\n",
    "\n",
    "\n",
    "def calculate_program_metrics(far, pd):\n",
    "\n",
    "    pd_at_far = 0.0\n",
    "    pd_at_eer = 0.0\n",
    "    far_at_eer = 0.0\n",
    "\n",
    "    for i in range(len(far)):\n",
    "      if far[i] > 0.1:\n",
    "        pd_at_far = pd[i-1]\n",
    "        break\n",
    "\n",
    "    for i in range(len(far)):\n",
    "      if pd[i] > 1 - far[i]:\n",
    "        pd_at_eer = (pd[i-1] + pd[i])/2\n",
    "        far_at_eer = (far[i-1] + far[i])/2\n",
    "        break\n",
    "    \n",
    "    \n",
    "    print(\"pD @ 0.1 FAR = %.3f\" % (pd_at_far))\n",
    "    print(\"pD @ EER = %.3f\" % (pd_at_eer))\n",
    "    print(\"FAR @ EER = %.3f\" % (far_at_eer))\n",
    "\n",
    "\n",
    "\n",
    "class GeneratedTextDetection:\n",
    "    \"\"\"\n",
    "    Artifact class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        torch.manual_seed(1000)\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        # Load the model from checkpoints\n",
    "        self.init_dict = self._init_detector()\n",
    "\n",
    "    def _init_detector(self):\n",
    "\n",
    "        init_dict = {\"kn_model\": None, \"kn_tokenizer\": None,\n",
    "                    \"unk_model\": None, \"unk_tokenizer\": None,\n",
    "                   \"attr_model\": None, \"attr_tokenizer\": None, }\n",
    "\n",
    "        if self.args.init_method == \"fused\":\n",
    "            model_name = 'roberta-large' if self.args.kn_large else 'roberta-base'\n",
    "            tokenization_utils.logger.setLevel('ERROR')\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "            lm = RobertaForFusion.from_pretrained(model_name).to(self.args.device)\n",
    "\n",
    "            model = FusedClassifier(lm=lm, device=self.args.device, FUSED_INPUT_SIZE=self.args.FUSED_INPUT_SIZE)\n",
    "            # Load the model from checkpoints\n",
    "            if self.args.device == \"cpu\":\n",
    "                model.load_state_dict(torch.load((self.args.check_point + '{}.pt').format(self.args.known_model_name),\n",
    "                                                 map_location='cpu')['model_state_dict'])\n",
    "            else:\n",
    "                print((self.args.check_point + '{}.pt').format(self.args.known_model_name))\n",
    "                model.load_state_dict(\n",
    "                    torch.load((self.args.check_point + '{}.pt').format(self.args.known_model_name))['model_state_dict'])\n",
    "            \n",
    "            init_dict[\"kn_model\"] = model\n",
    "            init_dict[\"kn_tokenizer\"] = tokenizer\n",
    "            return init_dict\n",
    "\n",
    "      \n",
    "    def evaluate(self, input_text):\n",
    "        \"\"\"\n",
    "           Method that runs the evaluation and generate scores and evidence\n",
    "        \"\"\"\n",
    "\n",
    "        # Encapsulate the inputs\n",
    "        eval_dataset = EncodeEvalData(input_text, self.init_dict[\"kn_tokenizer\"], self.args.special_puncts, self.args.max_sequence_length)\n",
    "        eval_loader = DataLoader(eval_dataset)\n",
    "\n",
    "        # Dictionary will contain all the scores and evidences generated by the model\n",
    "        results = {\"cls\": [], \"LLR_score\": [], \"prob_score\": {\"cls_0\": [], \"cls_1\": []}, \"generator\": None}\n",
    "\n",
    "        # Set eval mode\n",
    "        if self.args.init_method == \"fused\":\n",
    "            self.init_dict[\"kn_model\"].eval()\n",
    "\n",
    "      \n",
    "        with torch.no_grad():\n",
    "              for texts, masks, custom_features in eval_loader:\n",
    "                  texts, masks, custom_features = texts.to(self.args.device), masks.to(self.args.device), custom_features.to(self.args.device)\n",
    "\n",
    "                  if self.args.init_method == \"fused\":\n",
    "                      # Individual model take care all the probes\n",
    "                      output_dic = self.init_dict[\"kn_model\"](data=[texts, masks], custom_features = custom_features)\n",
    "                      disc_out = output_dic\n",
    "\n",
    "                      cls0_prob = disc_out[:, 0].tolist()\n",
    "                      cls1_prob = disc_out[:, 1].tolist()\n",
    "\n",
    "                      results[\"prob_score\"][\"cls_0\"].extend(cls0_prob)\n",
    "                      results[\"prob_score\"][\"cls_1\"].extend(cls1_prob)\n",
    "\n",
    "                      prior_llr = math.log10(self.args.kn_priors[0]/self.args.kn_priors[1])\n",
    "\n",
    "                      results[\"LLR_score\"].extend([math.log10(prob/(1-prob)) + prior_llr for prob in cls1_prob])\n",
    "\n",
    "                      _, predicted = torch.max(disc_out, 1)\n",
    "\n",
    "                      results[\"cls\"].extend(predicted.tolist())\n",
    "                     \n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Generated Text: Detection'\n",
    "    )\n",
    "\n",
    "    # Input data and files\n",
    "    parser.add_argument('--known_model_name', default=\"robertagenattr_fusion_grover\", type=str,\n",
    "                        help='name of the known generator detector model')\n",
    "\n",
    "    parser.add_argument('--init_method', default=\"fused\", type=str,\n",
    "                        help='name of the generator attribution model')\n",
    "\n",
    "    parser.add_argument('--check_point', default=\"/content/\", type=str,\n",
    "                        help='saved model checkpoint directory')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--device', type=str, default=None)\n",
    "\n",
    "    parser.add_argument('--kn_priors', type=list, default=[0.5, 0.5])\n",
    "    parser.add_argument('--unk_priors', type=list, default=[0.5, 0.5])\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=1)\n",
    "    parser.add_argument('--max-sequence-length', type=int, default=256)\n",
    "    parser.add_argument('--kn_large', type=bool, default=False)\n",
    "\n",
    "    parser.add_argument('--special_puncts', type=list, default=[\"!\",\"'\", \",\", \"-\", \":\", \";\", \"?\", \"@\", \"\\\"\", \"=\", \"#\"])\n",
    "    parser.add_argument('--FUSED_INPUT_SIZE', type=int, default=811)\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args=['--check_point=\"'])\n",
    "\n",
    "\n",
    "    if args.device is None:\n",
    "        args.device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "    predict_prob = []\n",
    "\n",
    "    y = []\n",
    "\n",
    "    artifact = GeneratedTextDetection(args)\n",
    "\n",
    "    test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    multiple_lines = 0\n",
    "\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    fp = 0  \n",
    "\n",
    "    for value in tqdm(test_data.itertuples()):\n",
    "      \n",
    "      if value.text.count(\"\\n\\n\\n\") > 0:\n",
    "        multiple_lines +=1\n",
    "      \n",
    "      main_body_text = value.text\n",
    "     \n",
    "\n",
    "      if main_body_text == \"\":\n",
    "        continue\n",
    "\n",
    "      results = artifact.evaluate([main_body_text])\n",
    "\n",
    "      y.append(value.label)\n",
    "\n",
    "\n",
    "      predict_prob.append(results[\"LLR_score\"][0])\n",
    "\n",
    "      predicted = results[\"cls\"][0]\n",
    "\n",
    "      tp += ((predicted == value.label) & (value.label == 1))\n",
    "      tn += ((predicted == value.label) & (value.label == 0))\n",
    "      fn += ((predicted != value.label) & (value.label == 1))\n",
    "      fp += ((predicted != value.label) & (value.label == 0))\n",
    "\n",
    "    recall = float(tp) / (tp+fn)\n",
    "    precision = float(tp) / (tp+fp)\n",
    "    f1_score = 2 * float(precision) * recall / (precision + recall)\n",
    "\n",
    "    print('TP: %d' % (\n",
    "        tp))\n",
    "    print('TN: %d' % (\n",
    "        tn))\n",
    "    print('FP: %d' % (\n",
    "        fp))\n",
    "    print('FN: %d' % (\n",
    "        fn))\n",
    "\n",
    "    print('Accuracy of the discriminator: %d %%' % (\n",
    "            100 * (tp + tn) / (tp + tn + fp + fn)))\n",
    "    print('Recall of the discriminator: %d %%' % (\n",
    "        100 * recall))\n",
    "    print('Precision of the discriminator: %d %%' % (\n",
    "        100 * precision))\n",
    "    print('f1_score of the discriminator: %d %%' % (\n",
    "        100 * f1_score))\n",
    "    \n",
    "\n",
    "    # calculate scores\n",
    "    lr_auc = roc_auc_score(y, predict_prob)\n",
    "\n",
    "    # summarize scores\n",
    "    print(\"\\n\")\n",
    "    print(\" ----- Extra Metrics -----\")\n",
    "    print()\n",
    "    print('Classifier: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "    # calculate roc curves\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y, predict_prob)\n",
    "\n",
    "    calculate_program_metrics(lr_fpr, lr_tpr)\n",
    "\n",
    "    eq_fpr = list(float_range(0, 1, 1 / len(lr_fpr)))\n",
    "    eq_tpr = [item for item in eq_fpr]\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Roberta')\n",
    "    pyplot.plot(eq_fpr, eq_tpr, marker='.', label='Random Chance')\n",
    "    # axis labels\n",
    "\n",
    "    pyplot.xlabel('Probability of False Alarm')\n",
    "    pyplot.ylabel('Probability of Detection')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "id": "2ajTcNFXbM5_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "3sw9PazDgrVQ"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "T1: Stylometry LM Fusion.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}