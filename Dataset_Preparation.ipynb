{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFd8nsqsckE_"
   },
   "source": [
    "## In-House Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9QKOvGsinlj"
   },
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFKkSSNPiuV_"
   },
   "outputs": [],
   "source": [
    "Data_folder = \"\" # Give the path of the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1AixYzokIeY"
   },
   "outputs": [],
   "source": [
    "split =['train', 'test']\n",
    "\n",
    "dataset =['covid', 'vaccine', 'climate']\n",
    "\n",
    "model = ['human','gpt2','gpt2-medium','gpt2-large','EleutherAI-gpt-neo-1.3B']\n",
    "\n",
    "timeline_length = [1, 10, 20, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJVT-Su6kuI-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def flatten_timeline(data_dir, split=\"test\", criteria=\"append\"):\n",
    "\n",
    "  for filename in tqdm(os.listdir(data_dir)):\n",
    "    \n",
    "    if \".json\" in filename:\n",
    "\n",
    "      if \"human\" in filename:\n",
    "\n",
    "        label = 1\n",
    "      \n",
    "      else:\n",
    "\n",
    "        label = 0\n",
    "\n",
    "      flatten_data_list = []\n",
    "      \n",
    "      info = filename.split(\"_\")\n",
    "\n",
    "      dataset = pd.read_json(data_dir + filename, lines=True, orient='record')\n",
    "\n",
    "      for index in dataset.columns:\n",
    "\n",
    "         tweets = dataset[[index]].values[0].tolist()[0]['tweets']\n",
    "\n",
    "         if criteria == \"append\":\n",
    "           flatten_tweet = \"\\n\".join(tweets)\n",
    "\n",
    "         flatten_data_list.append([index, flatten_tweet, label])\n",
    "      \n",
    "      flatten_frame = pd.DataFrame(flatten_data_list, columns =['index', 'text', 'label'])\n",
    "\n",
    "      new_filename = filename.replace(\".json\", \"\") + \".csv\"\n",
    "\n",
    "      flatten_frame.to_csv(data_dir+\"/Flatten/\"+new_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAdICB8jrSEq",
    "outputId": "236ff94e-02bc-413e-e846-cfe2b19253ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [02:19<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "flatten_timeline(Data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2"
   ],
   "metadata": {
    "id": "WHibqYVEgQMV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Data_folder = \"\""
   ],
   "metadata": {
    "id": "jArDaCiBgZwl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def flatten_timeline(data_dir, split=\"test\", criteria=\"frame\"):\n",
    "\n",
    "  for filename in tqdm(os.listdir(data_dir)):\n",
    "    \n",
    "    if \".json\" in filename:\n",
    "\n",
    "      if \"gpt\" in filename:\n",
    "\n",
    "        label = 0\n",
    "      \n",
    "      else:\n",
    "\n",
    "        label = 1\n",
    "\n",
    "      flatten_data_list = []\n",
    "      \n",
    "      info = filename.split(\"_\")\n",
    "\n",
    "      dataset = pd.read_json(data_dir + filename, lines=True, orient='record')\n",
    "\n",
    "      for index in dataset.columns:\n",
    "\n",
    "         tweets = dataset[[index]].values[0].tolist()[0]['tweets']\n",
    "          \n",
    "        #  tweets.reverse()\n",
    "\n",
    "         if criteria == \"append\":\n",
    "           flatten_tweet = \"\\n\".join(tweets)\n",
    "           \n",
    "         if criteria == \"frame\":\n",
    "           flatten_tweet = tweets\n",
    "          #  print(len(flatten_tweet))\n",
    "           \n",
    "         flatten_data_list.append([index, flatten_tweet, label])\n",
    "      \n",
    "      flatten_frame = pd.DataFrame(flatten_data_list, columns =['index', 'text', 'label'])\n",
    "\n",
    "      new_filename = filename.replace(\".json\", \"\") + \".csv\"\n",
    "\n",
    "      flatten_frame.to_csv(data_dir+\"/Flatten/Frame/\"+new_filename, index=False)\n"
   ],
   "metadata": {
    "id": "vBYw5Ru4gVg0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flatten_timeline(Data_folder)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LclikAYsg43-",
    "outputId": "f62476d9-30bb-46fb-fce9-8dbc4b4949e4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  5.51it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    ""
   ],
   "metadata": {
    "id": "rSUWI1oWJd6i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TweepFake Baseline"
   ],
   "metadata": {
    "id": "0pMDVTv-EVJR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_path = \"\"\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "OTgeeemZEYjP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tweepfake_train = pd.read_csv(data_path + \"train.csv\")"
   ],
   "metadata": {
    "id": "-aKCRgA2Jp8R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tweepfake_train[tweepfake_train['class_type'] ==\"human\"]\n",
    "\n",
    "new_data_row = []\n",
    "\n",
    "for row in tweepfake_train.itertuples():\n",
    "\n",
    "  if row.class_type == 'human':\n",
    "\n",
    "    new_data_row.append([row.text, 1])\n",
    "  \n",
    "  else:\n",
    "\n",
    "    new_data_row.append([row.text, 0])\n",
    "\n",
    "new_frame = pd.DataFrame(new_data_row, columns=['text', 'label'])\n",
    "\n",
    "new_frame.to_csv(data_path + \"train_new.csv\", index=False)"
   ],
   "metadata": {
    "id": "fHcDeNWiFmUg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tweepfake_train = pd.read_csv(data_path + \"train_new.csv\")\n",
    "tweepfake_test = pd.read_csv(data_path + \"test_new.csv\")\n",
    "tweepfake_validate = pd.read_csv(data_path + \"validation_new.csv\")"
   ],
   "metadata": {
    "id": "4uaI9APVnIlE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tweepfake_test"
   ],
   "metadata": {
    "id": "mI3hDTSKoCdy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "human_train_df = tweepfake_train[tweepfake_train.label == 1]\n",
    "human_test_df = tweepfake_test[tweepfake_test.label == 1]\n",
    "human_valid_df = tweepfake_validate[tweepfake_validate.label == 1]\n",
    "\n",
    "jsonl_data = human_train_df.to_json(orient='records', lines=True)\n",
    "\n",
    "with open(data_path+\"tweepfake_human.train.jsonl\", \"w\") as text_file:\n",
    "  text_file.write(jsonl_data)\n",
    "\n",
    "jsonl_data = human_test_df.to_json(orient='records', lines=True)\n",
    "\n",
    "with open(data_path+\"tweepfake_human.test.jsonl\", \"w\") as text_file:\n",
    "  text_file.write(jsonl_data)\n",
    "\n",
    "jsonl_data = human_valid_df.to_json(orient='records', lines=True)\n",
    "\n",
    "with open(data_path+\"tweepfake_human.valid.jsonl\", \"w\") as text_file:\n",
    "  text_file.write(jsonl_data)"
   ],
   "metadata": {
    "id": "_fSkaIzkn8qM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "machine_train_df = tweepfake_train[tweepfake_train.label == 0]\n",
    "machine_test_df = tweepfake_test[tweepfake_test.label == 0]\n",
    "machine_valid_df = tweepfake_validate[tweepfake_validate.label == 0]\n",
    "\n",
    "jsonl_data = machine_train_df.to_json(orient='records', lines=True)\n",
    "\n",
    "with open(data_path+\"tweepfake_machine.train.jsonl\", \"w\") as text_file:\n",
    "  text_file.write(jsonl_data)\n",
    "\n",
    "jsonl_data = machine_test_df.to_json(orient='records', lines=True)\n",
    "\n",
    "with open(data_path+\"tweepfake_machine.test.jsonl\", \"w\") as text_file:\n",
    "  text_file.write(jsonl_data)\n",
    "\n",
    "jsonl_data = machine_valid_df.to_json(orient='records', lines=True)\n",
    "\n",
    "with open(data_path+\"tweepfake_machine.valid.jsonl\", \"w\") as text_file:\n",
    "  text_file.write(jsonl_data)"
   ],
   "metadata": {
    "id": "5F_rR-Y4pDin"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Dataset Preparation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}